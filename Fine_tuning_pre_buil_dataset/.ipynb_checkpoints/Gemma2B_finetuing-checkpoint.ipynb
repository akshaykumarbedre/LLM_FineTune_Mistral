{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55f72f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (2.6.0+git684f6f2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (0.21.0+4040d51)\n",
      "Collecting torchao\n",
      "  Downloading torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting torchtune\n",
      "  Downloading torchtune-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (80.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch) (2025.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Collecting torchdata==0.11.0 (from torchtune)\n",
      "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting datasets (from torchtune)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface_hub[hf_transfer] (from torchtune)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors (from torchtune)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting kagglehub (from torchtune)\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting sentencepiece (from torchtune)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from torchtune)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting blobfile>=2 (from torchtune)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tokenizers (from torchtune)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (4.67.1)\n",
      "Collecting omegaconf (from torchtune)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (7.0.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: lxml>=4.9 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from blobfile>=2->torchtune) (5.3.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->torchtune)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (2.0.3)\n",
      "Collecting xxhash (from datasets->torchtune)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->torchtune)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.11.18)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->torchtune)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.10)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub[hf_transfer]->torchtune)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (2025.4.26)\n",
      "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]->torchtune)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->torchtune)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Downloading torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtune-0.6.1-py3-none-any.whl (910 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.7/910.7 kB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
      "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m304.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m198.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m259.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m170.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m282.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "\u001b[33m  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144589 sha256=9dd2a2ea93e7e11ddbb951b9e3ea63f71314f552d4c646bc324ccf423478f74b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: torchao, sentencepiece, antlr4-python3-runtime, xxhash, safetensors, regex, pycryptodomex, pyarrow, omegaconf, hf-xet, hf-transfer, fsspec, dill, tiktoken, multiprocess, kagglehub, huggingface_hub, blobfile, torchdata, tokenizers, datasets, torchtune\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [pyarrow]domex]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.0m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/22\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: dill\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/22\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.3.7━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/22\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.3.7:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/22\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.3.70m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/22\u001b[0m [fsspec]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [torchtune]22\u001b[0m [torchtune]e_hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed antlr4-python3-runtime-4.9.3 blobfile-3.0.0 datasets-4.0.0 dill-0.3.8 fsspec-2025.3.0 hf-transfer-0.1.9 hf-xet-1.1.5 huggingface_hub-0.34.3 kagglehub-0.3.12 multiprocess-0.70.16 omegaconf-2.3.0 pyarrow-21.0.0 pycryptodomex-3.23.0 regex-2025.7.34 safetensors-0.5.3 sentencepiece-0.2.0 tiktoken-0.9.0 tokenizers-0.21.4 torchao-0.12.0 torchdata-0.11.0 torchtune-0.6.1 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchao torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb13a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Ignoring files matching the following patterns: None\n",
      ".gitattributes: 100%|██████████████████████| 1.57k/1.57k [00:00<00:00, 21.4MB/s]\n",
      "README.md: 100%|████████████████████████████| 29.1k/29.1k [00:00<00:00, 174MB/s]\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 15.2MB/s]\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.35MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|████| 4.99G/4.99G [00:03<00:00, 1.49GB/s]\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:01<00:00, 183MB/s]\n",
      "model.safetensors.index.json: 100%|█████████| 24.2k/24.2k [00:00<00:00, 172MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 10.3MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 35.0MB/s]\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 15.6MB/s]\n",
      "tokenizer_config.json: 100%|████████████████| 47.0k/47.0k [00:00<00:00, 264MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/gemma-2-2b-it/README.md\n",
      "/tmp/gemma-2-2b-it/generation_config.json\n",
      "/tmp/gemma-2-2b-it/.gitattributes\n",
      "/tmp/gemma-2-2b-it/tokenizer_config.json\n",
      "/tmp/gemma-2-2b-it/tokenizer.json\n",
      "/tmp/gemma-2-2b-it/.cache\n",
      "/tmp/gemma-2-2b-it/special_tokens_map.json\n",
      "/tmp/gemma-2-2b-it/model-00001-of-00002.safetensors\n",
      "/tmp/gemma-2-2b-it/original_repo_id.json\n",
      "/tmp/gemma-2-2b-it/model-00002-of-00002.safetensors\n",
      "/tmp/gemma-2-2b-it/config.json\n",
      "/tmp/gemma-2-2b-it/model.safetensors.index.json\n",
      "/tmp/gemma-2-2b-it/tokenizer.model\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!tune download google/gemma-2-2b-it\\\n",
    "    --hf-token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cfba25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         phi4/14B_full_low_memory                \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         llama3_3/70B_full_multinode             \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         phi4/14B_full                           \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         phi4/14B_lora_single_device             \n",
      "                                         phi4/14B_qlora_single_device            \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "full_dpo_distributed                     llama3_1/8B_full_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         phi4/14B_lora                           \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "dev/lora_finetune_distributed_multi_dataset dev/11B_lora_multi_dataset              \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/generate_v2_distributed              llama3/70B_generation_distributed       \n",
      "                                         llama3_1/70B_generation_distributed     \n",
      "                                         llama3_3/70B_generation_distributed     \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         qwen2_5/evaluation                      \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi4/evaluation                         \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "                                         llama3_2/evaluation                     \n",
      "                                         code_llama2/evaluation                  \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "!tune ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506c7733-b2a5-482e-af1a-e851c3af2735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "output_dir: /tmp/torchtune/gemma2_2B/lora_single_device\n",
      "tokenizer:\n",
      "    _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "    path: /tmp/gemma-2-2b/tokenizer.model\n",
      "dataset:\n",
      "    _component_: torchtune.datasets.alpaca_dataset\n",
      "    packed: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "model:\n",
      "    _component_: torchtune.models.gemma2.lora_gemma2_2b\n",
      "    lora_attn_modules:\n",
      "    - q_proj\n",
      "    - v_proj\n",
      "    - output_proj\n",
      "    apply_lora_to_mlp: true\n",
      "    lora_rank: 64\n",
      "    lora_alpha: 128\n",
      "    lora_dropout: 0.0\n",
      "checkpointer:\n",
      "    _component_: torchtune.training.FullModelHFCheckpointer\n",
      "    checkpoint_dir: /tmp/gemma-2-2b/\n",
      "    checkpoint_files:\n",
      "    - model-00001-of-00003.safetensors\n",
      "    - model-00002-of-00003.safetensors\n",
      "    - model-00003-of-00003.safetensors\n",
      "    recipe_checkpoint: null\n",
      "    output_dir: ${output_dir}\n",
      "    model_type: GEMMA2\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "optimizer:\n",
      "    _component_: torch.optim.AdamW\n",
      "    fused: true\n",
      "    lr: 2e-5\n",
      "lr_scheduler:\n",
      "    _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "    num_warmup_steps: 10\n",
      "loss:\n",
      "    _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "batch_size: 8\n",
      "epochs: 1\n",
      "max_steps_per_epoch: null\n",
      "gradient_accumulation_steps: 8\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "device: cuda\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "dtype: bf16\n",
      "metric_logger:\n",
      "    _component_: torchtune.training.metric_logging.DiskLogger\n",
      "    log_dir: ${output_dir}/logs\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "profiler:\n",
      "    _component_: torchtune.training.setup_torch_profiler\n",
      "    enabled: false\n",
      "    output_dir: ${output_dir}/profiling_outputs\n",
      "    cpu: true\n",
      "    cuda: true\n",
      "    profile_memory: false\n",
      "    with_stack: false\n",
      "    record_shapes: true\n",
      "    with_flops: false\n",
      "    wait_steps: 5\n",
      "    warmup_steps: 5\n",
      "    active_steps: 2\n",
      "    num_cycles: 1\n"
     ]
    }
   ],
   "source": [
    "!tune cat gemma2/2B_lora_single_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 50\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/gemma-2-2b-it\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: GEMMA2\n",
      "  output_dir: /tmp/tune/gemma-2-2b-it\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  packed: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 10\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/gemma2_2B/lora_single_device/logs\n",
      "model:\n",
      "  _component_: torchtune.models.gemma2.lora_gemma2_2b\n",
      "  apply_lora_to_mlp: true\n",
      "  lora_alpha: 128\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 64\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-05\n",
      "output_dir: /tmp/torchtune/gemma2_2B/lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/gemma2_2B/lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 5\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  path: /tmp/gemma-2-2b-it/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 2803197410. Local seed is seed + rank = 2803197410 + 0\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "Writing logs to /tmp/torchtune/gemma2_2B/lora_single_device/logs/log_1753861705.txt\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "        GPU peak memory allocation: 5.26 GiB\n",
      "        GPU peak memory reserved: 5.40 GiB\n",
      "        GPU peak memory active: 5.26 GiB\n",
      "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
      "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|                                                                                               | 0/130 [00:00<?, ?it/s]/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "1|130|Loss: 3.403407096862793: 100%|██████████████████████████████████████████████████████| 130/130 [20:47<00:00,  9.44s/it]INFO:torchtune.utils._logging:Starting checkpoint save...\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.65 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/model-00001-of-00002.safetensors\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 0.22 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/model-00002-of-00002.safetensors\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.14 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/adapter_model.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.14 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/adapter_model.safetensors\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Checkpoint saved in 7.35 seconds.\n",
      "1|130|Loss: 3.403407096862793: 100%|██████████████████████████████████████████████████████| 130/130 [20:54<00:00,  9.65s/it]\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config gemma2/2B_lora_single_device \\\n",
    "    batch_size=80 \\\n",
    "    checkpointer.checkpoint_dir=/tmp/gemma-2-2b-it \\\n",
    "    checkpointer.checkpoint_files=\"[model-00001-of-00002.safetensors,model-00002-of-00002.safetensors]\" \\\n",
    "    tokenizer.path=/tmp/gemma-2-2b-it/tokenizer.model \\\n",
    "    checkpointer.output_dir=/tmp/tune/gemma-2-2b-it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune cp eleuther_eval custom_eval_config1.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9fd1eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "INFO:torchtune.utils._logging:Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/tune/gemma-2-2b-it/epoch_0\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: GEMMA2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: null\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.gemma2.gemma2_2b\n",
      "output_dir: ./\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- truthfulqa_mc2\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: /tmp/tune/gemma-2-2b-it/epoch_0/tokenizer.model\n",
      "\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
      "config.json: 100%|█████████████████████████████| 665/665 [00:00<00:00, 5.86MB/s]\n",
      "tokenizer_config.json: 100%|██████████████████| 26.0/26.0 [00:00<00:00, 454kB/s]\n",
      "vocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 8.54MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 29.7MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 35.8MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 548M/548M [00:01<00:00, 506MB/s]\n",
      "generation_config.json: 100%|██████████████████| 124/124 [00:00<00:00, 2.12MB/s]\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "README.md: 9.59kB [00:00, 50.7MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|██████| 271k/271k [00:00<00:00, 109MB/s]\n",
      "Generating validation split: 100%|█| 817/817 [00:00<00:00, 150026.11 examples/s]\n",
      "INFO:torchtune.utils._logging:Running evaluation on the following tasks: ['truthfulqa_mc2']\n",
      "100%|███████████████████████████████████████| 817/817 [00:00<00:00, 1262.42it/s]\n",
      "Running loglikelihood requests: 100%|██████| 5882/5882 [00:42<00:00, 139.12it/s]\n",
      "INFO:torchtune.utils._logging:Eval completed in 45.51 seconds.\n",
      "INFO:torchtune.utils._logging:Max memory allocated: 28.89 GB\n",
      "INFO:torchtune.utils._logging:\n",
      "\n",
      "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_mc2|      3|none  |     0|acc   |↑  |0.4577|±  | 0.017|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tune run eleuther_eval --config ./custom_eval_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fcff48b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3febf8434ad34e49b681808dfdd85b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#TODO: update it to your chosen epoch\n",
    "trained_model_path = \"/tmp/tune/gemma-2-2b-it/epoch_0/\"\n",
    "\n",
    "# Define the model and adapter paths\n",
    "original_model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(original_model_name)\n",
    "\n",
    "# huggingface will look for adapter_model.safetensors and adapter_config.json\n",
    "peft_model = PeftModel.from_pretrained(model, trained_model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, prompt, max_length=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d87d9cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output: Identify the odd one out.: Twitter, Instagram, Telegram, Facebook, WhatsApp\n",
      "\n",
      "**Answer:** Telegram\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Twitter, Instagram, Facebook, and WhatsApp** are primarily social networking platforms focused on sharing content, connecting with friends and family, and building online communities.\n",
      "* **Telegram** is a messaging app that emphasizes privacy, security, and group communication. While it has social features, its core functionality is different from the other platforms listed. \n",
      "\n",
      "\n",
      "Let me know if you'd like to try another odd one out puzzle! \n",
      "\n",
      "Expected output: Telegram\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_input = \"Identify the odd one out.: Twitter, Instagram, Telegram\"\n",
    "expected_output = \"Telegram\"\n",
    "\n",
    "print(\"Base model output:\", generate_text(peft_model, tokenizer, test_input))\n",
    "print(\"Expected output:\", expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898897a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193b853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
