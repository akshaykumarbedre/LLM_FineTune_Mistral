{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c77f11-5add-4a7a-b7b3-f3ba32b98cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Ignoring files matching the following patterns: None\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 18.8MB/s]\n",
      "README.md: 100%|███████████████████████████| 1.55k/1.55k [00:00<00:00, 20.9MB/s]\n",
      "config.json: 100%|█████████████████████████████| 571/571 [00:00<00:00, 2.87MB/s]\n",
      "generation_config.json: 100%|██████████████████| 116/116 [00:00<00:00, 1.35MB/s]\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.26G/9.94G [00:07<00:05, 789MB/s]/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "model-00001-of-00002.safetensors: 100%|█████| 9.94G/9.94G [00:13<00:00, 726MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|████| 4.54G/4.54G [00:03<00:00, 1.41GB/s]\n",
      "model.safetensors.index.json: 100%|████████| 25.1k/25.1k [00:00<00:00, 11.2MB/s]\n",
      "pytorch_model-00001-of-00002.bin: 100%|████| 9.94G/9.94G [00:08<00:00, 1.18GB/s]\n",
      "pytorch_model-00002-of-00002.bin: 100%|████| 5.06G/5.06G [00:03<00:00, 1.34GB/s]\n",
      "pytorch_model.bin.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 5.97MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 5.93MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 14.7MB/s]\n",
      "tokenizer.model: 100%|███████████████████████| 493k/493k [00:00<00:00, 2.07MB/s]\n",
      "tokenizer_config.json: 100%|███████████████████| 996/996 [00:00<00:00, 13.1MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/Mistral-7B-v0.1/README.md\n",
      "/tmp/Mistral-7B-v0.1/generation_config.json\n",
      "/tmp/Mistral-7B-v0.1/.gitattributes\n",
      "/tmp/Mistral-7B-v0.1/tokenizer_config.json\n",
      "/tmp/Mistral-7B-v0.1/tokenizer.json\n",
      "/tmp/Mistral-7B-v0.1/.cache\n",
      "/tmp/Mistral-7B-v0.1/special_tokens_map.json\n",
      "/tmp/Mistral-7B-v0.1/pytorch_model-00001-of-00002.bin\n",
      "/tmp/Mistral-7B-v0.1/model-00001-of-00002.safetensors\n",
      "/tmp/Mistral-7B-v0.1/original_repo_id.json\n",
      "/tmp/Mistral-7B-v0.1/pytorch_model-00002-of-00002.bin\n",
      "/tmp/Mistral-7B-v0.1/model-00002-of-00002.safetensors\n",
      "/tmp/Mistral-7B-v0.1/config.json\n",
      "/tmp/Mistral-7B-v0.1/model.safetensors.index.json\n",
      "/tmp/Mistral-7B-v0.1/pytorch_model.bin.index.json\n",
      "/tmp/Mistral-7B-v0.1/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!tune download mistralai/Mistral-7B-v0.1\\\n",
    "    --hf-token <hf token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee061f9-4d86-419f-8a16-697a0ac74fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Ignoring files matching the following patterns: None\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/gemma-2-2b-it/README.md\n",
      "/tmp/gemma-2-2b-it/generation_config.json\n",
      "/tmp/gemma-2-2b-it/.gitattributes\n",
      "/tmp/gemma-2-2b-it/tokenizer_config.json\n",
      "/tmp/gemma-2-2b-it/tokenizer.json\n",
      "/tmp/gemma-2-2b-it/.cache\n",
      "/tmp/gemma-2-2b-it/special_tokens_map.json\n",
      "/tmp/gemma-2-2b-it/model-00001-of-00002.safetensors\n",
      "/tmp/gemma-2-2b-it/original_repo_id.json\n",
      "/tmp/gemma-2-2b-it/model-00002-of-00002.safetensors\n",
      "/tmp/gemma-2-2b-it/config.json\n",
      "/tmp/gemma-2-2b-it/model.safetensors.index.json\n",
      "/tmp/gemma-2-2b-it/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!tune download google/gemma-2-2b-it\\\n",
    "    --hf-token <hf token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53337174-f891-4b7c-8e2c-36a6a1e11783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "output_dir: /tmp/torchtune/gemma_2B/lora\n",
      "tokenizer:\n",
      "    _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "    path: /tmp/gemma-2b/tokenizer.model\n",
      "dataset:\n",
      "    _component_: torchtune.datasets.alpaca_dataset\n",
      "    packed: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "model:\n",
      "    _component_: torchtune.models.gemma.lora_gemma_2b\n",
      "    lora_attn_modules:\n",
      "    - q_proj\n",
      "    - v_proj\n",
      "    - output_proj\n",
      "    apply_lora_to_mlp: true\n",
      "    lora_rank: 64\n",
      "    lora_alpha: 128\n",
      "    lora_dropout: 0.0\n",
      "checkpointer:\n",
      "    _component_: torchtune.training.FullModelHFCheckpointer\n",
      "    checkpoint_dir: /tmp/gemma-2b/\n",
      "    checkpoint_files:\n",
      "    - model-00001-of-00002.safetensors\n",
      "    - model-00002-of-00002.safetensors\n",
      "    recipe_checkpoint: null\n",
      "    output_dir: ${output_dir}\n",
      "    model_type: GEMMA\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "optimizer:\n",
      "    _component_: torch.optim.AdamW\n",
      "    fused: true\n",
      "    lr: 2e-5\n",
      "lr_scheduler:\n",
      "    _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "    num_warmup_steps: 10\n",
      "loss:\n",
      "    _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "batch_size: 4\n",
      "epochs: 1\n",
      "max_steps_per_epoch: null\n",
      "gradient_accumulation_steps: 1\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "device: cuda\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "dtype: bf16\n",
      "metric_logger:\n",
      "    _component_: torchtune.training.metric_logging.DiskLogger\n",
      "    log_dir: ${output_dir}/logs\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "profiler:\n",
      "    _component_: torchtune.training.setup_torch_profiler\n",
      "    enabled: false\n",
      "    output_dir: ${output_dir}/profiling_outputs\n",
      "    cpu: true\n",
      "    cuda: true\n",
      "    profile_memory: false\n",
      "    with_stack: false\n",
      "    record_shapes: true\n",
      "    with_flops: false\n",
      "    wait_steps: 5\n",
      "    warmup_steps: 3\n",
      "    active_steps: 2\n",
      "    num_cycles: 1\n"
     ]
    }
   ],
   "source": [
    "!tune cat gemma/2B_lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e95cfa67-3b1b-4096-aceb-017906a86370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540865f3f82f4e898a3d5cfe2c84f914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What is the answer to life?  [/INST] The answer is 42. [INST] That's ridiculous  [/INST] Oh I know.\n",
      "[1, 733, 16289, 28793, 1824, 349, 272, 4372, 298, 1411, 28804, 28705, 733, 28748, 16289, 28793, 415, 4372, 349, 28705, 28781, 28750, 28723, 2, 1, 733, 16289, 28793, 1725, 28742, 28713, 16706, 28705, 733, 28748, 16289, 28793, 5434, 315, 873, 28723, 2]\n"
     ]
    }
   ],
   "source": [
    "#loading from dataset fri data.json sharegpt formate \n",
    "from torchtune.models.mistral import mistral_tokenizer\n",
    "from torchtune.datasets import chat_dataset\n",
    "\n",
    "m_tokenizer = mistral_tokenizer(\n",
    "    path=\"/tmp/Mistral-7B-v0.1/tokenizer.model\",\n",
    "    prompt_template=\"torchtune.models.mistral.MistralChatTemplate\",\n",
    "    max_seq_len=8192,\n",
    ")\n",
    "ds = chat_dataset(\n",
    "    tokenizer=m_tokenizer,\n",
    "    source=\"json\",\n",
    "    data_files=\"data/chat_dataset.json\",\n",
    "    split=\"train\",\n",
    "    conversation_column=\"conversations\",\n",
    "    conversation_style=\"sharegpt\",\n",
    "    # By default, user prompt is ignored in loss. Set to True to include it\n",
    "    train_on_input=True,\n",
    "    new_system_prompt=None,\n",
    ")\n",
    "tokenized_dict = ds[0]\n",
    "tokens, labels = tokenized_dict[\"tokens\"], tokenized_dict[\"labels\"]\n",
    "print(m_tokenizer.decode(tokens))\n",
    "# [INST] What is the answer to life?  [/INST] The answer is 42. [INST] That's ridiculous  [/INST] Oh I know.\n",
    "print(labels)\n",
    "# [1, 733, 16289, 28793, 1824, 349, 272, 4372, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42af3c82-67aa-4845-8b42-f8fa2d298e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading  data from hugging face \n",
    "from torchtune.models.gemma import gemma_tokenizer\n",
    "from torchtune.datasets import chat_dataset\n",
    "\n",
    "g_tokenizer = gemma_tokenizer(\"/tmp/gemma-2-2b-it/tokenizer.model\")\n",
    "ds = chat_dataset(\n",
    "    tokenizer=g_tokenizer,\n",
    "    source=\"Open-Orca/SlimOrca-Dedup\",\n",
    "    conversation_column=\"conversations\",\n",
    "    conversation_style=\"sharegpt\",\n",
    "    split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66a16f58-7dc0-4d8a-8bc6-847824c3f6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an AI assistant. You will be given a task. You must generate a detailed and long answer.Write an article based on this \"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal – blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims – adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community\\'s Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community\\'s response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_tokenizer.decode(ds[0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64198073-2c9f-48b1-a694-19b64ae59f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35379646243e43bcb81a71b8a7167fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant.Correct this to standard English:This are a cat\n",
      "---\n",
      "Corrected:This is a cat.\n",
      "[-100, -100, -100, -100, -100, -100, -100, 27957, 736, 577, 5039, 4645, 235292, 1596, 708, 476, 4401, 108, 3976, 108, 198286, 235292, 1596, 603, 476, 4401, 235265, 1]\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.gemma import gemma_tokenizer\n",
    "from torchtune.datasets import instruct_dataset\n",
    "\n",
    "g_tokenizer = gemma_tokenizer(\n",
    "    path=\"/tmp/gemma-2-2b-it/tokenizer.model\",\n",
    "    prompt_template=\"torchtune.data.GrammarErrorCorrectionTemplate\",\n",
    "    max_seq_len=8192,\n",
    ")\n",
    "ds = instruct_dataset(\n",
    "    tokenizer=g_tokenizer,\n",
    "    source=\"csv\",\n",
    "    data_files=\"data/instruct_dataset.csv\",\n",
    "    split=\"train\",\n",
    "    # By default, user prompt is ignored in loss. Set to True to include it\n",
    "    train_on_input=True,\n",
    "    # Prepend a system message to every sample\n",
    "    new_system_prompt=\"You are an AI assistant. \",\n",
    "    # Use columns in our dataset instead of default\n",
    "    column_map={\"input\": \"incorrect\", \"output\": \"correct\"},\n",
    ")\n",
    "tokenized_dict = ds[0]\n",
    "tokens, labels = tokenized_dict[\"tokens\"], tokenized_dict[\"labels\"]\n",
    "print(g_tokenizer.decode(tokens))\n",
    "# You are an AI assistant. Correct this to standard English:This are a cat---\\nCorrected:This is a cat.\n",
    "print(labels)  # System message is masked out, but not user message\n",
    "# [-100, -100, -100, -100, -100, -100, 27957, 736, 577, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd4532a6-9fb8-49e7-a268-39b5044f9799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant.Correct this to standard English:Helio\n",
      "---\n",
      "Corrected: hello\n",
      "[-100, -100, -100, -100, -100, -100, -100, 27957, 736, 577, 5039, 4645, 235292, 1949, 4167, 108, 3976, 108, 198286, 235292, 25612, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dict = ds[1]\n",
    "tokens, labels = tokenized_dict[\"tokens\"], tokenized_dict[\"labels\"]\n",
    "print(g_tokenizer.decode(tokens))\n",
    "# You are an AI assistant. Correct this to standard English:This are a cat---\\nCorrected:This is a cat.\n",
    "print(labels)  # System message is masked out, but not user message\n",
    "# [-100, -100, -100, -100, -100, -100, 27957, 736, 577, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f4b9a80-d177-4df6-8e0f-cf7328f301c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4649867b5adf42ff92d47c6637ff9e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In code\n",
    "from torchtune.models.gemma import gemma_tokenizer\n",
    "from torchtune.datasets import instruct_dataset\n",
    "\n",
    "g_tokenizer = gemma_tokenizer(\"/tmp/gemma-2-2b/tokenizer.model\")\n",
    "ds = instruct_dataset(\n",
    "    tokenizer=g_tokenizer,\n",
    "    source=\"json\",\n",
    "    data_files=\"data/instruct_data.json\",\n",
    "    column_map={\"input\": \"prompt\", \"output\": \"response\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "157bfd2a-ed83-4193-ab8b-cbb91a119eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello worldbye world'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_tokenizer.decode(ds[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d48311a-40c0-4c87-a580-6451c77e82f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66641760846146999aa2dcab5d0fcccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In code\n",
    "from torchtune.models.gemma import gemma_tokenizer\n",
    "from torchtune.datasets import instruct_dataset\n",
    "\n",
    "g_tokenizer = gemma_tokenizer(\"/tmp/gemma-2-2b/tokenizer.model\")\n",
    "ds = instruct_dataset(\n",
    "    tokenizer=g_tokenizer,\n",
    "    source=\"json\",\n",
    "    data_files=\"data/database.json\",\n",
    "    column_map={\"input\": \"input\", \"output\": \"output\"},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e34ac01f-1f6e-4f66-9f21-c9e2a6822c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which customers from Pune have placed an order in the last month?To answer this, you need to query the `customers` and `orders` tables.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_tokenizer.decode(ds[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ea6258-e506-4c45-ac07-3294680deee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Ignoring files matching the following patterns: None\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 21.6MB/s]\n",
      "LICENSE: 100%|█████████████████████████████| 7.80k/7.80k [00:00<00:00, 74.9MB/s]\n",
      "README.md: 100%|███████████████████████████| 38.9k/38.9k [00:00<00:00, 5.09MB/s]\n",
      "USE_POLICY.md: 100%|███████████████████████| 4.70k/4.70k [00:00<00:00, 62.3MB/s]\n",
      "config.json: 100%|█████████████████████████████| 654/654 [00:00<00:00, 12.2MB/s]\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 2.86MB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████| 4.98G/4.98G [00:06<00:00, 732MB/s]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "model-00002-of-00004.safetensors: 100%|█████| 5.00G/5.00G [00:05<00:00, 893MB/s]\n",
      "model-00003-of-00004.safetensors: 100%|████| 4.92G/4.92G [00:04<00:00, 1.01GB/s]\n",
      "model-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:02<00:00, 487MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 6.71MB/s]\n",
      "original/consolidated.00.pth: 100%|█████████| 16.1G/16.1G [00:18<00:00, 871MB/s]\n",
      "params.json: 100%|█████████████████████████████| 211/211 [00:00<00:00, 3.43MB/s]\n",
      "original/tokenizer.model: 100%|████████████| 2.18M/2.18M [00:00<00:00, 4.85MB/s]\n",
      "special_tokens_map.json: 100%|███████████████| 73.0/73.0 [00:00<00:00, 1.42MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 49.9MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 39.2MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/Meta-Llama-3-8B-Instruct/LICENSE\n",
      "/tmp/Meta-Llama-3-8B-Instruct/README.md\n",
      "/tmp/Meta-Llama-3-8B-Instruct/model-00002-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "/tmp/Meta-Llama-3-8B-Instruct/.gitattributes\n",
      "/tmp/Meta-Llama-3-8B-Instruct/tokenizer_config.json\n",
      "/tmp/Meta-Llama-3-8B-Instruct/model-00001-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3-8B-Instruct/tokenizer.json\n",
      "/tmp/Meta-Llama-3-8B-Instruct/.cache\n",
      "/tmp/Meta-Llama-3-8B-Instruct/special_tokens_map.json\n",
      "/tmp/Meta-Llama-3-8B-Instruct/original_repo_id.json\n",
      "/tmp/Meta-Llama-3-8B-Instruct/USE_POLICY.md\n",
      "/tmp/Meta-Llama-3-8B-Instruct/original\n",
      "/tmp/Meta-Llama-3-8B-Instruct/model-00003-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3-8B-Instruct/config.json\n",
      "/tmp/Meta-Llama-3-8B-Instruct/model-00004-of-00004.safetensors\n",
      "/tmp/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "!tune download meta-llama/Meta-Llama-3-8B-Instruct\\\n",
    "    --hf-token <hf token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0f4c6a-fd1d-4137-bf58-23912121465e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20a1011d08c461ab75afc024f1ad219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01e16ef2b574589a873d9d4f5520814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)egpt4v_instruct_gpt4-vision_cap100k.json:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e621855ab83431c9a1b837d2061915f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/102025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In code\n",
    "from torchtune.models.llama3_2_vision import llama3_2_vision_transform\n",
    "from torchtune.datasets.multimodal import multimodal_chat_dataset\n",
    "\n",
    "model_transform = llama3_2_vision_transform(\n",
    "    path=\"/tmp/Meta-Llama-3-8B-Instruct/original/tokenizer.model\",\n",
    "    max_seq_len=8192,\n",
    "    image_size=560,\n",
    ")\n",
    "ds = multimodal_chat_dataset(\n",
    "    model_transform=model_transform,\n",
    "    source=\"Lin-Chen/ShareGPT4V\",\n",
    "    split=\"train\",\n",
    "    name=\"ShareGPT4V\",\n",
    "    image_dir=\"/home/user/dataset/\",\n",
    "    image_tag=\"<image>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d52938-ec14-4198-be82-6125e6936d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
