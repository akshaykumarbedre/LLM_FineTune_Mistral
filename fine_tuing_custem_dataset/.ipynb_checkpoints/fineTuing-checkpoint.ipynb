{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140783c9-189b-4ac9-bf75-02ff5df7e1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Ignoring files matching the following patterns: None\n",
      ".gitattributes: 100%|██████████████████████| 1.57k/1.57k [00:00<00:00, 22.1MB/s]\n",
      "README.md: 100%|████████████████████████████| 29.1k/29.1k [00:00<00:00, 165MB/s]\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 16.0MB/s]\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.19MB/s]\n",
      "model-00001-of-00002.safetensors:  48%|█▉  | 2.42G/4.99G [00:02<00:02, 1.27GB/s]/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "model-00001-of-00002.safetensors: 100%|████| 4.99G/4.99G [00:03<00:00, 1.62GB/s]\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:00<00:00, 384MB/s]\n",
      "model.safetensors.index.json: 100%|█████████| 24.2k/24.2k [00:00<00:00, 190MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 9.43MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 57.2MB/s]\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 8.28MB/s]\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 1.13MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/gemma-2-2b-it/README.md\n",
      "/tmp/gemma-2-2b-it/generation_config.json\n",
      "/tmp/gemma-2-2b-it/.gitattributes\n",
      "/tmp/gemma-2-2b-it/tokenizer_config.json\n",
      "/tmp/gemma-2-2b-it/tokenizer.json\n",
      "/tmp/gemma-2-2b-it/.cache\n",
      "/tmp/gemma-2-2b-it/special_tokens_map.json\n",
      "/tmp/gemma-2-2b-it/model-00001-of-00002.safetensors\n",
      "/tmp/gemma-2-2b-it/original_repo_id.json\n",
      "/tmp/gemma-2-2b-it/model-00002-of-00002.safetensors\n",
      "/tmp/gemma-2-2b-it/config.json\n",
      "/tmp/gemma-2-2b-it/model.safetensors.index.json\n",
      "/tmp/gemma-2-2b-it/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "!tune download google/gemma-2-2b-it\\\n",
    "    --hf-token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd11b7f-7ef4-4594-a224-ff10ce702557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         phi4/14B_full_low_memory                \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         llama3_3/70B_full_multinode             \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         phi4/14B_full                           \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         phi4/14B_lora_single_device             \n",
      "                                         phi4/14B_qlora_single_device            \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "full_dpo_distributed                     llama3_1/8B_full_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         phi4/14B_lora                           \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "dev/lora_finetune_distributed_multi_dataset dev/11B_lora_multi_dataset              \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/generate_v2_distributed              llama3/70B_generation_distributed       \n",
      "                                         llama3_1/70B_generation_distributed     \n",
      "                                         llama3_3/70B_generation_distributed     \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         qwen2_5/evaluation                      \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi4/evaluation                         \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "                                         llama3_2/evaluation                     \n",
      "                                         code_llama2/evaluation                  \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a91376-d530-4f88-b7c5-8badb5de8a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Copied file to gemma2bconfig.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune cp  gemma2/2B_lora_single_device gemma2bconfig.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e497f3-b245-4cb1-a57b-095298920f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 15\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /tmp/gemma-2-2b-it\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00002.safetensors\n",
      "  - model-00002-of-00002.safetensors\n",
      "  model_type: GEMMA2\n",
      "  output_dir: /tmp/tune/gemma-2-2b-it\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "  column_map:\n",
      "    input: input\n",
      "    output: output\n",
      "  data_files: data/database.json\n",
      "  packed: false\n",
      "  source: json\n",
      "  split: train\n",
      "  train_on_input: true\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 80\n",
      "gradient_accumulation_steps: 8\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 10\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /tmp/torchtune/gemma2_2B/lora_single_device/logs\n",
      "model:\n",
      "  _component_: torchtune.models.gemma2.lora_gemma2_2b\n",
      "  apply_lora_to_mlp: true\n",
      "  lora_alpha: 512\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 256\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-05\n",
      "output_dir: /tmp/torchtune/gemma2_2B/lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: /tmp/torchtune/gemma2_2B/lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 5\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.gemma.gemma_tokenizer\n",
      "  path: /tmp/gemma-2-2b-it/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 2382139440. Local seed is seed + rank = 2382139440 + 0\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "Writing logs to /tmp/torchtune/gemma2_2B/lora_single_device/logs/log_1753938137.txt\n",
      "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 5.74 GiB\n",
      "\tGPU peak memory reserved: 5.79 GiB\n",
      "\tGPU peak memory active: 5.74 GiB\n",
      "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
      "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "0it [00:00, ?it/s]/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "INFO:torchtune.utils._logging:Starting checkpoint save...\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.65 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/model-00001-of-00002.safetensors\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 0.22 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/model-00002-of-00002.safetensors\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.58 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/adapter_model.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.58 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/adapter_model.safetensors\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_0/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.00 GiB saved to /tmp/tune/gemma-2-2b-it/recipe_state/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Checkpoint saved in 10.88 seconds.\n",
      "\n",
      "0it [00:19, ?it/s]\u001b[A\n",
      "INFO:torchtune.utils._logging:Starting checkpoint save...\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.65 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_1/model-00001-of-00002.safetensors\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 0.22 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_1/model-00002-of-00002.safetensors\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.58 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_1/adapter_model.pt\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.58 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_1/adapter_model.safetensors\n",
      "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /tmp/tune/gemma-2-2b-it/epoch_1/adapter_config.json\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 0.00 GiB saved to /tmp/tune/gemma-2-2b-it/recipe_state/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Checkpoint saved in 7.97 seconds.\n",
      "0it [00:08, ?it/s]\n",
      "INFO:torchtune.utils._logging:Starting checkpoint save...\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config gemma2/2B_lora_single_device batch_size=10 \\\n",
    "    checkpointer.checkpoint_dir=/tmp/gemma-2-2b-it \\\n",
    "    checkpointer.checkpoint_files=\"[model-00001-of-00002.safetensors,model-00002-of-00002.safetensors]\" \\\n",
    "    tokenizer.path=/tmp/gemma-2-2b-it/tokenizer.model \\\n",
    "    checkpointer.output_dir=/tmp/tune/gemma-2-2b-it \\\n",
    "    dataset.source=json \\\n",
    "    dataset.data_files=data/database.json\\\n",
    "    dataset.split=train \\\n",
    "    dataset.train_on_input=True \\\n",
    "    dataset.column_map.input=input \\\n",
    "    dataset.column_map.output=output \\\n",
    "    epochs=100 \\\n",
    "    model.lora_rank=256 \\\n",
    "    model.lora_alpha=512 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfb98b-e2fd-4a15-ad28-5f8b95ac63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#TODO: update it to your chosen epoch\n",
    "trained_model_path = \"/tmp/tune/gemma-2-2b-it/epoch_39\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=trained_model_path,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_model_path, safetensors=True)\n",
    "\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, prompt, max_length=1000):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10ad7c98-2e3d-427f-8abd-3c001320c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output: You are a database agent. Based on the user's question, identify the database tables required to find the answer. Do not write the SQL query. Just list the tables, query : What are the top 5 most sold products by quantity.\n",
      "\n",
      "\n",
      "Answer:\n",
      "```\n",
      "Products\n",
      "Sales\n",
      "```\n",
      "```\n",
      "Products\n",
      "Sales\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"You are a database agent. Based on the user's question, identify the database tables required to find the answer. Do not write the SQL query. Just list the tables, \\\n",
    "query : What are the top 5 most sold products by quantity.\"\n",
    "print(\"Base model output:\", generate_text(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b25dfb-848a-43d2-92e0-bb664891474d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
