{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKadZFQ2IdJb"
      },
      "source": [
        "# Fine-tuning with the Hugging Face ecosystem (TRL)\n",
        "\n",
        "_Authored by: [Sergio Paniego](https://github.com/sergiopaniego) and modified by [AMD](https://www.amd.com)_ to run on AMD GPUs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12AmRCbVly_C"
      },
      "source": [
        "This notebook demonstrates how to fine-tune a [Vision Language Model (VLM)](https://huggingface.co/blog/vlms) using the Hugging Face ecosystem, specifically with the [Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/index) and [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index) libraries.\n",
        "\n",
        "**Note**: This notebook is derived from [fine_tuning_vlm_trl](https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl).\n",
        "\n",
        "## Model and dataset overview\n",
        "\n",
        "You’ll be fine-tuning the [Qwen2-VL-7B](https://qwenlm.github.io/blog/qwen2-vl/) model on the [ChartQA](https://huggingface.co/datasets/HuggingFaceM4/ChartQA) dataset. This dataset includes images of various chart types paired with question-answer pairs, which is ideal for enhancing the model's visual question-answering capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phEjDPRvly_D"
      },
      "source": [
        "### 1. Install the dependencies\n",
        "\n",
        "Verify the Torch library is installed and GPUs are accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCMhPmFdIGSb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "import torch\n",
        "print(\"Is a ROCm-GPU detected? \", torch.cuda.is_available())\n",
        "print(\"How many ROCm-GPUs are detected? \", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwKCRo9bly_D"
      },
      "source": [
        "Then use `pip` to install the following dependencies for the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96hPGHg0ly_D"
      },
      "outputs": [],
      "source": [
        "# Upgrade pip, setuptools, wheel\n",
        "!python3.12 -m pip install --upgrade --force-reinstall --no-cache-dir pip setuptools wheel\n",
        "\n",
        "# Install everything except numpy\n",
        "!pip install transformers==4.47.0 trl==0.12.0 peft==0.13.2 qwen-vl-utils==0.0.8 wandb==0.19.1 accelerate==1.1.1 ipywidgets==8.1.5 numba\n",
        "\n",
        "# Then install numpy\n",
        "!pip install numpy==1.25.* --only-binary=:all:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcKkh_YQly_D"
      },
      "source": [
        "Verify the installation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr_xPOhNly_D"
      },
      "outputs": [],
      "source": [
        "# Check for the required libraries and their versions\n",
        "!pip list | grep -E \"transformers|trl|peft|qwen-vl-utils|wandb|accelerate|ipywidgets|numpy|numba\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0-2Lso6wkIh"
      },
      "source": [
        "### 2. Provide your Hugging Face token\n",
        "\n",
        "Install Hugging Face if neccessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qnE2nKCly_E"
      },
      "source": [
        "Log in to Hugging Face to upload your fine-tuned model. You’ll need to authenticate with your Hugging Face account to save and share your model directly from this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcL4-bwGIoaR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import huggingface_hub\n",
        "    print(\"huggingface_hub is already installed.\")\n",
        "except ImportError:\n",
        "    !pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObPyHAEZly_E"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_HGCjczly_E"
      },
      "source": [
        "Verify that your token was accepted correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4y_YpDGly_E"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "try:\n",
        "    api = HfApi()\n",
        "    user_info = api.whoami()\n",
        "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Token validation failed. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9QXwbJ7ovM5"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "In this section, you’ll load the [HuggingFaceM4/ChartQA](https://huggingface.co/datasets/HuggingFaceM4/ChartQA) dataset. This dataset contains chart images paired with related questions and answers, making it ideal for training on visual question-answering tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBWqnXkcTN-s"
      },
      "source": [
        "Next, generate a system message for the VLM. This creates a system that acts as an expert in analyzing chart images and providing concise answers to questions based on these charts.\n",
        "\n",
        "**⚠️ Important: ensure the correct kernel is selected**\n",
        "\n",
        "If the verification process fails, ensure the correct Jupyter kernel is selected for your notebook.\n",
        "To change the kernel, follow these steps:\n",
        "\n",
        "1. Go to the **Kernel** menu.\n",
        "2. Select **Change Kernel**\n",
        "3. Select `Python 3 (ipykernel)` from the list.\n",
        "\n",
        "**Failure to select the correct kernel can lead to unexpected issues when running the notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBvKAlXhI46X"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are a Vision Language Model specializing in interpreting visual data from chart images.\n",
        "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
        "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
        "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8UpL5ZxTakm"
      },
      "source": [
        "Format the dataset into a chatbot structure for interaction. Each interaction consists of a system message, followed by the image and the user's query, and finally, the answer to the query.\n",
        "\n",
        "For more usage tips specific to this model, see the [Model Card](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct#more-usage-tips)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG8NuzqjjbgI"
      },
      "outputs": [],
      "source": [
        "def format_data(sample):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": system_message\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": sample[\"image\"],\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample['query'],\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": sample[\"label\"][0]\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ2zuDeGThNe"
      },
      "source": [
        "For educational purposes, you’ll only load 10% of each split in the dataset. However, in a real-world use case, you would typically load the entire set of samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFe_A78aIwK8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"HuggingFaceM4/ChartQA\"\n",
        "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=['train[:10%]', 'val[:10%]', 'test[:10%]'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0X4ZajcPV10"
      },
      "source": [
        "Next, look at the structure of the dataset. It includes an image, a query, a label (which is the answer), and a fourth feature that you’ll be discarding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2UKZj15jGwv"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1edUqNGWTtjA"
      },
      "source": [
        "Format the data using the chatbot structure. This sets up the interactions appropriately for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSHNqk0dkxii"
      },
      "outputs": [],
      "source": [
        "train_dataset = [format_data(sample) for sample in train_dataset]\n",
        "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
        "test_dataset = [format_data(sample) for sample in test_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlp3WnfEuftT"
      },
      "outputs": [],
      "source": [
        "train_dataset[200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY1Y_KDtoycB"
      },
      "source": [
        "## Load the model and check its performance\n",
        "\n",
        "After you’ve loaded the dataset, load the model and evaluate its performance using a sample from the dataset. This tutorial uses [Qwen/Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct), a Vision Language Model (VLM) capable of understanding both visual data and text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCJhM6tCw4lq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
        "\n",
        "model_id = \"Qwen/Qwen2-VL-7B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HobU2iPUDWL"
      },
      "source": [
        "Next, load the model and the tokenizer to prepare for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awtjIq86JfFF"
      },
      "outputs": [],
      "source": [
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "processor = Qwen2VLProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JtKcuuXUGUT"
      },
      "source": [
        "To evaluate the model's performance, use a sample from the dataset. First, examine the internal structure of this sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-eIIdL9lqJJ"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLaWWJk_RkVU"
      },
      "source": [
        "Use the sample without the system message to assess the VLM's raw understanding. Here’s the input to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytnr1rePOamM"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][1:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IK2HOMuRtY_"
      },
      "source": [
        "Now review the chart corresponding to the sample. Can you answer the query based on the visual information?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QavnLzjJUbxf"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][1]['content'][0]['image']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpLfsCUtUW6I"
      },
      "source": [
        "Create a method that takes the model, processor, and sample as inputs to generate the model's answer. This allows you to streamline the inference process and easily evaluate the VLM's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MoRTjFcE8qD"
      },
      "outputs": [],
      "source": [
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
        "    # Prepare the text input by applying the chat template\n",
        "    text_input = processor.apply_chat_template(\n",
        "        sample[1:2],  # Use the sample without the system message\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Process the visual input from the sample\n",
        "    image_inputs, _ = process_vision_info(sample)\n",
        "\n",
        "    # Prepare the inputs for the model\n",
        "    model_inputs = processor(\n",
        "        text=[text_input],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)  # Move inputs to the specified device\n",
        "\n",
        "    # Generate text with the model\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Trim the generated ids to remove the input ids\n",
        "    trimmed_generated_ids = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # Decode the output text\n",
        "    output_text = processor.batch_decode(\n",
        "        trimmed_generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "\n",
        "    return output_text[0]  # Return the first decoded output text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UeNiMJC_uCk"
      },
      "outputs": [],
      "source": [
        "# Example of how to call the method with sample:\n",
        "output = generate_text_from_sample(model, processor, train_dataset[0], device=\"cuda\")\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysh0e9DRUfF-"
      },
      "source": [
        "While the model successfully retrieves the correct visual information, it struggles to answer the question accurately. This indicates that fine-tuning might be the key to enhancing its performance. It's now time to proceed with the fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3b76rawti6"
      },
      "source": [
        "### Remove the model and clean the GPU\n",
        "\n",
        "Before proceeding to train the model in the next section, clear the current variables and clean the GPU to free up resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxkXZuUkvy8j"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "def clear_memory():\n",
        "    # Delete variables if they exist in the current global scope\n",
        "    if 'inputs' in globals(): del globals()['inputs']\n",
        "    if 'model' in globals(): del globals()['model']\n",
        "    if 'processor' in globals(): del globals()['processor']\n",
        "    if 'trainer' in globals(): del globals()['trainer']\n",
        "    if 'peft_model' in globals(): del globals()['peft_model']\n",
        "    if 'bnb_config' in globals(): del globals()['bnb_config']\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Garbage collection and clearing CUDA memory\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    time.sleep(2)\n",
        "    gc.collect()\n",
        "    time.sleep(2)\n",
        "\n",
        "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIZOIVEzQqNg"
      },
      "source": [
        "## Fine-tune the model using TRL\n",
        "\n",
        "Follow these steps to fine-tune your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrR9gP2z90z"
      },
      "source": [
        "### 1. Load the model for training\n",
        "\n",
        "First, load the original model.  \n",
        "\n",
        "**Note**: Alternatively, the quantized model could be loaded by using [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index). To learn more about quantization, see [this blog post from Hugging Face](https://huggingface.co/blog/merve/quantization) or [one from Maarten Grootendorst](https://www.maartengrootendorst.com/blog/quantization/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm_bJRrXsESg"
      },
      "outputs": [],
      "source": [
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ").half()\n",
        "processor = Qwen2VLProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65wfO29isQlX"
      },
      "source": [
        "### 2. Set up LoRA and SFTConfig\n",
        "\n",
        "Next, configure LoRA for the training setup. LoRA, which reduces memory usage by applying a low-rank approximation, leads to even lower memory requirements and improved training efficiency, making it an excellent choice for optimizing the model's performance without sacrificing quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITmkRHWCKYjf"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply PEFT model adaptation\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5zzHM2GVtxD"
      },
      "source": [
        "Use Supervised Fine-tuning (SFT) to refine the model’s performance on the task. To do this, define the training arguments using the [SFTConfig](https://huggingface.co/docs/trl/sft_trainer) class from the [TRL library](https://huggingface.co/docs/trl/index). SFT provides labeled data, helping the model learn to generate more accurate responses based on its input. This approach ensures the model is tailored to your specific use case, leading to better performance in understanding and responding to visual queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbqX1pQUKaSM"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "# Configure training arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"qwen2-7b-instruct-trl-sft-ChartQA\",  # Directory to save the model\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=4,  # Batch size for training\n",
        "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
        "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
        "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
        "    # Optimizer and scheduler settings\n",
        "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
        "    # optim = \"adamw_hf\",\n",
        "    learning_rate=2e-4,  # Learning rate for training\n",
        "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
        "    # Logging and evaluation\n",
        "    logging_steps=1,  # Steps interval for logging\n",
        "    eval_steps=10,  # Steps interval for evaluation\n",
        "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
        "    save_strategy=\"steps\",  # Strategy for saving the model\n",
        "    save_steps=20,  # Steps interval for saving\n",
        "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
        "    greater_is_better=False,  # Whether higher metric values are better\n",
        "    load_best_model_at_end=True,  # Load the best model after training\n",
        "    # Mixed precision and gradient settings\n",
        "    bf16=False,  # Use bfloat16 precision\n",
        "    fp16=True,  # Use float16 precision\n",
        "    tf32=False,  # Use TensorFloat-32 precision\n",
        "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
        "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
        "    # Hub and reporting\n",
        "    push_to_hub=False,  # Whether to push model to Hugging Face Hub, disable it by default.\n",
        "    report_to=None,  # Reporting tool for tracking metrics\n",
        "    # Gradient checkpointing settings\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
        "    # Dataset configuration\n",
        "    dataset_text_field=\"\",  # Text field in dataset\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
        "    #max_seq_length=1024  # Maximum sequence length for input\n",
        ")\n",
        "\n",
        "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOUrD9P-y-Kf"
      },
      "source": [
        "### 3. Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjQGt-iZVyef"
      },
      "source": [
        "You can log your training progress using [Weights & Biases (W&B)](https://wandb.ai/). Connect your notebook to W&B to capture essential information during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckVfXDWsoF4Y"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"qwen2-7b-instruct-trl-sft-ChartQA\",  # change this\n",
        "    name=\"qwen2-7b-instruct-trl-sft-ChartQA\",  # change this\n",
        "    config=training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucTUbGURV2_-"
      },
      "source": [
        "The model needs a collator function to properly retrieve and batch the data during the training procedure. This function formats the dataset inputs for the model, ensuring they are correctly structured. Define the collator function below.\n",
        "\n",
        "For more details, see the TRL example [scripts]( https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py#L87).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAzDovzylQeZ"
      },
      "outputs": [],
      "source": [
        "# Create a data collator to encode text and image pairs\n",
        "def collate_fn(examples):\n",
        "    # Get the texts and images, and apply the chat template\n",
        "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]  # Prepare texts for processing\n",
        "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)  # Encode texts and images into tensors\n",
        "\n",
        "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
        "\n",
        "    # Ignore the image token index in the loss computation (model specific)\n",
        "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
        "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
        "    else:\n",
        "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
        "\n",
        "    # Mask image token IDs in the labels\n",
        "    for image_token_id in image_tokens:\n",
        "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
        "\n",
        "    batch[\"labels\"] = labels  # Add labels to the batch\n",
        "\n",
        "    return batch  # Return the prepared batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skbpTuJlV8qN"
      },
      "source": [
        "Now, define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer), which is a wrapper around the [Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class and inherits its attributes and methods. This class simplifies the fine-tuning process by properly initializing the [PeftModel](https://huggingface.co/docs/peft/v0.6.0/package_reference/peft_model) when a [PeftConfig](https://huggingface.co/docs/peft/v0.6.0/en/package_reference/config#peft.PeftConfig) object is provided. By using `SFTTrainer`, you can efficiently manage the training workflow and ensure a smooth fine-tuning experience for the Vision Language Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_jk-U7ULYtA"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=processor.tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlDsh4WvWCx0"
      },
      "source": [
        "It's now time to train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1rgMTBDLboO"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6CykSCtX-Xa"
      },
      "source": [
        "Then save the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE8usZw0lgrL"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yx_sGW42dN3"
      },
      "source": [
        "## Testing the fine-tuned model\n",
        "\n",
        "Now that you've successfully fine-tuned your Vision Language Model (VLM), it's time to evaluate its performance. This section tests the model using examples from the ChartQA dataset to see how well it answers questions based on chart images. It provides a good way to explore the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0KEPu6qYKqn"
      },
      "source": [
        "Clean up the GPU memory to ensure optimal performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttx6EK8Uy8t0"
      },
      "outputs": [],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwCTPHsfujn2"
      },
      "source": [
        "Then reload the base model using the same pipeline as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFqTNUud2lA7"
      },
      "outputs": [],
      "source": [
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "processor = Qwen2VLProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRAPEYKkYSkB"
      },
      "source": [
        "Attach the trained adapter to the pretrained model. This adapter contains the fine-tuning adjustments you made during training, allowing the base model to leverage the new knowledge without altering its core parameters. Integrating the adapter enhances the model's capabilities while maintaining its original structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzPCWG72ly_L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQi2xBXk4sHe"
      },
      "outputs": [],
      "source": [
        "adapter_path = \"./qwen2-7b-instruct-trl-sft-ChartQA\"\n",
        "model.load_adapter(adapter_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqryChyLWRmR"
      },
      "source": [
        "Use the previous sample from the dataset that the model initially struggled to answer correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X9YGJUezZr6"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hLJrxxTVn6x"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][1]['content'][0]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdb9vErmzdAf"
      },
      "outputs": [],
      "source": [
        "output = generate_text_from_sample(model, processor, train_dataset[0])\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swibyq5AWctZ"
      },
      "source": [
        "This sample is drawn from the training set, so the model has encountered it during training. This could be seen as a form of cheating. To gain a more comprehensive understanding of the model's performance, evaluate it using an unseen sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czZSBgnoef1E"
      },
      "outputs": [],
      "source": [
        "test_dataset[10][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATuQ6ZS6eirO"
      },
      "outputs": [],
      "source": [
        "test_dataset[10][1]['content'][0]['image']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yHJMKHNWcMc"
      },
      "outputs": [],
      "source": [
        "output = generate_text_from_sample(model, processor, test_dataset[10])\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUr6jmnAIlh1"
      },
      "source": [
        "The model has successfully learned to respond to the queries as specified in the dataset. You've achieved your goal!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daUMWw5xxhSc"
      },
      "source": [
        "## Compare a fine-tuned model versus a base model with prompting\n",
        "\n",
        "You have explored how fine-tuning the VLM can be a valuable option for adapting it to your specific needs. Another approach to consider is directly using prompting or implementing a RAG system, which is covered in another [recipe](https://huggingface.co/learn/cookbook/multimodal_rag_using_document_retrieval_and_vlms).\n",
        "\n",
        "Fine-tuning a VLM requires significant amounts of data and computational resources, which can incur costs. In contrast, you can experiment with prompting to see if it can achieve similar results without the overhead of fine-tuning.\n",
        "\n",
        "Clean up the GPU memory again to ensure optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei-OZGGx4lHe"
      },
      "outputs": [],
      "source": [
        "clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NApMx5S4-sh"
      },
      "source": [
        "First, load the baseline model following the same pipeline as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbdSklNAR1q-"
      },
      "outputs": [],
      "source": [
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "processor = Qwen2VLProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bwatIlf5EDE"
      },
      "source": [
        "This case uses the previous sample again, but this time it includes the system message as follows. This enhancement helps contextualize the input for the model, potentially improving its response accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNMKKvvZxqR8"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fou6XKGM5Uii"
      },
      "source": [
        "Now see how it performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN3NkkpgR4do"
      },
      "outputs": [],
      "source": [
        "text = processor.apply_chat_template(\n",
        "    train_dataset[0][:2], tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "image_inputs, _ = process_vision_info(train_dataset[0])\n",
        "\n",
        "inputs = processor(\n",
        "    text=[text],\n",
        "    images=image_inputs,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "inputs = inputs.to(\"cuda\")\n",
        "\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
        "\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "output_text[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Id3dzV5Wwy"
      },
      "source": [
        "As demonstrated, the model generates the correct answer using the pretrained model along with the additional system message, without any training. This approach can serve as a viable alternative to fine-tuning, depending on the specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Happy coding! If you encounter issues or have questions, don’t hesitate to ask or raise an issue on our [Github page](https://github.com/ROCm/gpuaidev)!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
