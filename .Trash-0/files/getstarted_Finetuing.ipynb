{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5c5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchao\n",
      "  Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m209.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m173.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [torchvision]\u001b[0m [torchvision]ver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pillow-11.3.0 setuptools-80.9.0 sympy-1.14.0 torch-2.7.1 torchao-0.12.0 torchvision-0.22.1 triton-3.3.1 typing-extensions-4.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c9bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtune in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (0.6.1)\n",
      "Requirement already satisfied: torchdata==0.11.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.11.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.34.3)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.5.3)\n",
      "Requirement already satisfied: kagglehub in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.3.12)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.9.0)\n",
      "Requirement already satisfied: blobfile>=2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (0.21.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (1.26.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (4.67.1)\n",
      "Requirement already satisfied: omegaconf in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (7.0.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchtune) (11.0.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.6.0+git684f6f2)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.23.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from blobfile>=2->torchtune) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (80.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->torchtune) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (2025.4.26)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (from tiktoken->torchtune) (2025.7.34)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c61ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tune [-h] {download,ls,cp,run,validate,cat} ...\n",
      "\n",
      "Welcome to the torchtune CLI!\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "subcommands:\n",
      "  {download,ls,cp,run,validate,cat}\n",
      "    download            Download a model from the Hugging Face Hub or Kaggle\n",
      "                        Model Hub.\n",
      "    ls                  List all built-in recipes and configs\n",
      "    cp                  Copy a built-in recipe or config to a local path.\n",
      "    run                 Run a recipe. For distributed recipes, this supports\n",
      "                        all torchrun arguments.\n",
      "    validate            Validate a config and ensure that it is well-formed.\n",
      "    cat                 Pretty print a config, making it easy to know which\n",
      "                        parameters you can override with `tune run`.\n"
     ]
    }
   ],
   "source": [
    "!tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "299c2930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Ignoring files matching the following patterns: None\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/Llama-2-7b-hf/README.md\n",
      "/tmp/Llama-2-7b-hf/generation_config.json\n",
      "/tmp/Llama-2-7b-hf/.gitattributes\n",
      "/tmp/Llama-2-7b-hf/tokenizer_config.json\n",
      "/tmp/Llama-2-7b-hf/tokenizer.json\n",
      "/tmp/Llama-2-7b-hf/LICENSE.txt\n",
      "/tmp/Llama-2-7b-hf/.cache\n",
      "/tmp/Llama-2-7b-hf/special_tokens_map.json\n",
      "/tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin\n",
      "/tmp/Llama-2-7b-hf/model-00001-of-00002.safetensors\n",
      "/tmp/Llama-2-7b-hf/original_repo_id.json\n",
      "/tmp/Llama-2-7b-hf/USE_POLICY.md\n",
      "/tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin\n",
      "/tmp/Llama-2-7b-hf/model-00002-of-00002.safetensors\n",
      "/tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
      "/tmp/Llama-2-7b-hf/config.json\n",
      "/tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
      "/tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
      "/tmp/Llama-2-7b-hf/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "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!tune download meta-llama/Llama-2-7b-hf \\\n",
    "  --output-dir /tmp/Llama-2-7b-hf \\\n",
    "  --hf-token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ae0dc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         phi4/14B_full_low_memory                \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         llama3_3/70B_full_multinode             \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         phi4/14B_full                           \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         phi4/14B_lora_single_device             \n",
      "                                         phi4/14B_qlora_single_device            \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "full_dpo_distributed                     llama3_1/8B_full_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         phi4/14B_lora                           \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "dev/lora_finetune_distributed_multi_dataset dev/11B_lora_multi_dataset              \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/generate_v2_distributed              llama3/70B_generation_distributed       \n",
      "                                         llama3_1/70B_generation_distributed     \n",
      "                                         llama3_3/70B_generation_distributed     \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         qwen2_5/evaluation                      \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi4/evaluation                         \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "                                         llama3_2/evaluation                     \n",
      "                                         code_llama2/evaluation                  \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2be2d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "Copied file to custom_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune  cp llama2/7B_lora_single_device custom_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d67a2bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared-docker\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "806d804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /opt/conda/envs/py_3.12/lib/python3.12/site-packages (6.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09c23f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': '/tmp/torchtune/llama2_7B/lora_single_device', 'model': {'_component_': 'torchtune.models.llama2.lora_llama2_7b', 'lora_attn_modules': ['q_proj', 'v_proj', 'output_proj'], 'apply_lora_to_mlp': True, 'apply_lora_to_output': False, 'lora_rank': 8, 'lora_alpha': 16, 'lora_dropout': 0.0}, 'tokenizer': {'_component_': 'torchtune.models.llama2.llama2_tokenizer', 'path': '/tmp/Llama-2-7b-hf/tokenizer.model', 'max_seq_len': None}, 'checkpointer': {'_component_': 'torchtune.training.FullModelHFCheckpointer', 'checkpoint_dir': '/tmp/Llama-2-7b-hf', 'checkpoint_files': ['pytorch_model-00001-of-00002.bin', 'pytorch_model-00002-of-00002.bin'], 'adapter_checkpoint': None, 'recipe_checkpoint': None, 'output_dir': '${output_dir}', 'model_type': 'LLAMA2'}, 'resume_from_checkpoint': False, 'save_adapter_weights_only': False, 'dataset': {'_component_': 'torchtune.datasets.alpaca_cleaned_dataset', 'packed': False}, 'seed': None, 'shuffle': True, 'batch_size': 2, 'optimizer': {'_component_': 'torch.optim.AdamW', 'fused': True, 'weight_decay': 0.01, 'lr': '3e-4'}, 'lr_scheduler': {'_component_': 'torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup', 'num_warmup_steps': 100}, 'loss': {'_component_': 'torchtune.modules.loss.CEWithChunkedOutputLoss'}, 'epochs': 1, 'max_steps_per_epoch': None, 'gradient_accumulation_steps': 8, 'clip_grad_norm': None, 'compile': False, 'metric_logger': {'_component_': 'torchtune.training.metric_logging.DiskLogger', 'log_dir': '${output_dir}/logs'}, 'log_every_n_steps': 1, 'log_peak_memory_stats': True, 'device': 'cuda', 'dtype': 'bf16', 'enable_activation_checkpointing': True, 'enable_activation_offloading': False, 'profiler': {'_component_': 'torchtune.training.setup_torch_profiler', 'enabled': False, 'output_dir': '${output_dir}/profiling_outputs', 'cpu': True, 'cuda': True, 'profile_memory': False, 'with_stack': False, 'record_shapes': True, 'with_flops': False, 'wait_steps': 5, 'warmup_steps': 5, 'active_steps': 2, 'num_cycles': 1}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "file=\"custom_config.yaml\"\n",
    "with open(file,'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "074a1d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config llama2/7B_lora_single_device epochs=1 batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "738f8eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0d36b5328f4c31abaac5b06463980f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f35a091d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cff91f1a4a4341ba1b835520fabf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output: What is the capital of France?\n",
      " Write the name of the capital of France.\n",
      "What is the name of the currency in France?\n",
      "Write the name of the currency in France.\n",
      "What is the name of the currency in France?\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#TODO: update it to your chosen epoch\n",
    "trained_model_path = \"/tmp/torchtune/llama2_7B/lora_single_device/epoch_0\"\n",
    "\n",
    "# Define the model and adapter paths\n",
    "original_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(original_model_name)\n",
    "\n",
    "# huggingface will look for adapter_model.safetensors and adapter_config.json\n",
    "peft_model = PeftModel.from_pretrained(model, trained_model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "print(\"Base model output:\", generate_text(peft_model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6341a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
